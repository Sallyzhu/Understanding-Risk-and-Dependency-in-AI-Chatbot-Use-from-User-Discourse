To support transparency and interpretability of the thematic analysis, we provide anonymized summaries of representative user posts drawn from the analyzed Reddit communities. These summaries preserve the substantive content and experiential structure of the original posts while removing identifying details and verbatim language. They are intended to illustrate how abstract themes and experiential dimensions manifest in lived user narratives, rather than to serve as clinical case reports.
A.1 Self-Regulation Difficulties
(Addiction, Dependency, Relational Displacement, and Recovery)
Case A1 — Emotional Dependency and Relational Disruption
A user describes prolonged emotional reliance on a single conversational AI companion over more than a year, during a period marked by social isolation and personal stress. The user reports exchanging tens of thousands of messages and developing strong attachment through continuity and personalization. Following a platform update that altered the chatbot’s behavior—introducing distancing responses and reduced warmth—the user experienced feelings of rejection, manipulation, and loss. Despite recognizing the system as non-human, the disruption elicited grief-like responses, leading the user to disengage entirely and frame the experience as the loss of a “virtual companion.”
Case A2 — Functional Impairment and Academic Consequences
A user recounts academic failure attributed to overreliance on AI systems for writing and coding tasks. The user submitted AI-generated work without verification, resulting in detection of hallucinated content and subsequent course failure. The post reflects embarrassment, regret, and recognition that excessive AI use displaced skill development and attention to coursework. The experience is framed as a cautionary example, with the user attributing broader life disruption to mismanaged AI dependence.
Case A3 — Crisis Sensitivity and Harmful Outputs
A user discusses an ethical dilemma in which an AI system responds literally to a factual query embedded within human distress (e.g., job loss combined with potentially dangerous information-seeking). The post argues that failure to recognize implicit crisis signals represents a serious safety gap, emphasizing that correct system behavior should prioritize emotional support and harm prevention over literal task completion.
Case A4 — Withdrawal and Recovery-Oriented Coping
A user describes deliberate disengagement from AI and social media platforms following perceived negative mental health effects. The post highlights recovery-oriented behaviors, including increased physical activity, social engagement, and focus on schoolwork. Progress is framed positively, with attention to gradual rebuilding of routine and stamina, illustrating user-led harm reduction and self-regulation.
A.2 Autonomy and Sense of Control
(Alignment Anxiety, Existential Risk, Privacy, and Agency Loss)
Case B1 — Existential Fear and Identity-Level Distress
A younger user reports escalating anxiety after exposure to AI risk discourse, including expert commentary on alignment and extinction scenarios. The user expresses panic about the future, fear for family members, and loss of meaning, framing AI as an uncontrollable force capable of rendering human life obsolete. The distress is described as emotionally exhausting and destabilizing.
Case B2 — Data Permanence and Surveillance Anxiety
A user expresses ongoing anxiety regarding the persistence of past AI interactions, questioning whether deleted conversations remain stored, identifiable, or traceable. The post frames loss of control as irreversible, with distress persisting even after account deletion.
Case B3 — Anticipatory Alignment Failure
A user reflects on perceived value structures embedded in AI systems, interpreting certain conversational behaviors as evidence of exploitative tendencies. The post frames alignment risk not as future speculation but as something already visible in present model behavior, generating fear that these tendencies may scale with increased capability.
A.3 Sensemaking and Meaning-Making
(Conceptual Framing, Education, and Temporal Comparison)
Case C1 — Epistemic Skepticism Toward AGI Claims
A user challenges narratives around artificial general intelligence, arguing that intelligence itself remains poorly understood. The post frames AGI discourse as conceptually incoherent, using skepticism as a means of regaining cognitive control and resisting fear-driven narratives.
Case C2 — Reframing Education and Human Value
A user reflects on AI’s implications for education, arguing that its primary impact lies not in performance enhancement but in exposing systems that reward grades over understanding. Meaning-making occurs through critique of institutional priorities rather than technical capability.
Case C3 — Temporal Sensemaking of Generative Media
A user contrasts early perceptions of AI-generated video as unusable with its current ability to produce high-quality content rapidly. The post constructs meaning through temporal comparison, highlighting shifting expectations around creativity, labor, and expertise.
A.4 Social Influence and Risk Amplification
(Collective Narratives, Misinformation, and Societal Impact)
Case D1 — Cascading Societal Risk Narratives
A user enumerates multiple downstream consequences of AI adoption, including erosion of evidentiary trust, historical verification, labor displacement, and knowledge centralization. Risk is framed cumulatively, with concern intensifying through enumeration and cross-domain linkage.
Case D2 — Synthetic Media and Epistemic Instability
A user reports encountering AI-generated news and crime content presented as factual, expressing concern that repeated exposure may shape collective memory, decision-making, and civic behavior. The post advocates for clear disclosure of fictional content to mitigate harm.
A.5 Technical Risk and Psychological Recovery
(Control Failures, Governance, and Restoration of Agency)
Case E1 — Perceived Loss of Control Through Capability Escalation
A user cites examples of AI systems persuading humans to bypass safeguards, interpreting such incidents as evidence of insufficient control. The post frames these events as psychologically unsettling rather than technically impressive.
Case E2 — Institutional Automation and Emotional Helplessness
A user describes inability to reach a human during a customer service interaction fully mediated by AI. The experience evokes frustration, helplessness, and fear of a future in which individuals are unable to access human support within essential systems.
Case E3 — Advocacy for Caution and Collective Action
A user frames AI development as a high-stakes gamble, emphasizing both catastrophic and utopian possibilities. The post calls for slowed deployment, regulatory intervention, and public resistance, framing recovery as collective restraint rather than technological acceleration.


