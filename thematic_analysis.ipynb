{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29d73a3c",
   "metadata": {},
   "source": [
    "#add GPT API key here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141e8e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json, re\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# ========= ÂÆûÈ™åÂèÇÊï∞ =========\n",
    "TEXT_COL = \"selftext\"\n",
    "N_SAMPLE = 80          # ÈÄâÊã©ÂÆûÈ™åÊù°Êï∞\n",
    "MIN_WORDS = 75\n",
    "MAX_WORDS = 500\n",
    "\n",
    "THEMATIC_DEV_ROUNDS = 2   # Phase 1‚Äì2 Â§öËΩÆÊ¨°Êï∞\n",
    "APPLY_BATCH_SIZE = 8      # Phase 6 batch size\n",
    "MODEL = \"gpt-5-mini\"\n",
    "\n",
    "def run_thematic_analysis_with_checkpoint(\n",
    "    csv_path,\n",
    "    save_path,\n",
    "    text_column=\"selftext\",\n",
    "    dev_n=50,\n",
    "    apply_batch_size=5,\n",
    "    min_words=10,\n",
    "    max_words=600,\n",
    "):\n",
    "    import os\n",
    "    import json\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    from collections import defaultdict\n",
    "    from crewai import Agent, Task, Crew, Process\n",
    "\n",
    "    # =====================================================\n",
    "    # Utility functions\n",
    "    # =====================================================\n",
    "\n",
    "    def parse_json(text, default=None):\n",
    "        if default is None:\n",
    "            default = []\n",
    "        if not text:\n",
    "            return default\n",
    "        text = text.strip()\n",
    "        text = re.sub(r\"^```json\", \"\", text)\n",
    "        text = re.sub(r\"```$\", \"\", text)\n",
    "        starts = [i for i in [text.find(\"{\"), text.find(\"[\")] if i != -1]\n",
    "        if not starts:\n",
    "            return default\n",
    "        first = min(starts)\n",
    "        last = max(text.rfind(\"}\"), text.rfind(\"]\")) + 1\n",
    "        try:\n",
    "            return json.loads(text[first:last])\n",
    "        except Exception:\n",
    "            return default\n",
    "\n",
    "    def filter_by_word_count(text):\n",
    "        if not isinstance(text, str):\n",
    "            return None\n",
    "        words = text.split()\n",
    "        if len(words) < min_words:\n",
    "            return None\n",
    "        if len(words) > max_words:\n",
    "            words = words[:max_words]\n",
    "        return \" \".join(words)\n",
    "\n",
    "    def split_long_post(text, max_chars=1500):\n",
    "        if not isinstance(text, str):\n",
    "            return []\n",
    "        paragraphs = [p.strip() for p in text.split(\"\\n\") if p.strip()]\n",
    "        chunks, current = [], \"\"\n",
    "        for p in paragraphs:\n",
    "            if len(current) + len(p) <= max_chars:\n",
    "                current = current + \" \" + p if current else p\n",
    "            else:\n",
    "                chunks.append(current)\n",
    "                current = p\n",
    "        if current:\n",
    "            chunks.append(current)\n",
    "        return chunks\n",
    "\n",
    "    def aggregate_codes_to_post(segment_codes):\n",
    "        post_codes = defaultdict(set)\n",
    "        for item in segment_codes:\n",
    "            if not isinstance(item, dict):\n",
    "                continue\n",
    "            pid = item.get(\"post_id\")\n",
    "            codes = item.get(\"codes\")\n",
    "            if pid is None or not isinstance(codes, list):\n",
    "                continue\n",
    "            for c in codes:\n",
    "                c = str(c).strip()\n",
    "                if c:\n",
    "                    post_codes[str(pid)].add(c)\n",
    "        return [\n",
    "            {\"post_id\": pid, \"codes\": sorted(list(codes))}\n",
    "            for pid, codes in post_codes.items()\n",
    "        ]\n",
    "\n",
    "    # =====================================================\n",
    "    # Load data\n",
    "    # =====================================================\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    #df=df.head(120)\n",
    "    if text_column not in df.columns:\n",
    "        raise ValueError(f\"CSV ÂøÖÈ°ªÂåÖÂê´Âàó: {text_column}\")\n",
    "\n",
    "    df = df[df[text_column].notna()]\n",
    "    df[text_column] = df[text_column].astype(str)\n",
    "\n",
    "    df[\"post_id\"] = df.index.astype(str)\n",
    "    df[\"posts\"] = df[text_column]\n",
    "\n",
    "    print(f\"üì• Loaded {len(df)} raw posts\")\n",
    "\n",
    "    # =====================================================\n",
    "    # Phase 1‚Äì5: exploratory multi-agent theme construction\n",
    "    # =====================================================\n",
    "\n",
    "    dev_df = df.sample(min(dev_n, len(df)), random_state=42)\n",
    "\n",
    "    dev_segments = []\n",
    "    for row in dev_df.itertuples():\n",
    "        for i, part in enumerate(split_long_post(row.posts)):\n",
    "            dev_segments.append({\n",
    "                \"segment_id\": f\"{row.post_id}_seg{i}\",\n",
    "                \"post_id\": row.post_id,\n",
    "                \"text\": part\n",
    "            })\n",
    "\n",
    "    familiarizer = Agent(\n",
    "        role=\"Familiarization Agent\",\n",
    "        goal=\"Read text segments and write analytic memos.\",\n",
    "        backstory=\"You are a qualitative researcher immersing yourself in raw text.\",\n",
    "        llm=\"gpt-5-mini\",\n",
    "        allow_delegation=False,\n",
    "    )\n",
    "\n",
    "    coder = Agent(\n",
    "        role=\"Coding Agent\",\n",
    "        goal=\"Generate semantic codes.\",\n",
    "        backstory=\"You perform open and reflexive qualitative coding.\",\n",
    "        llm=\"gpt-5-mini\",\n",
    "        allow_delegation=False,\n",
    "    )\n",
    "\n",
    "    theme_builder = Agent(\n",
    "        role=\"Theme Builder\",\n",
    "        goal=\"Develop and define themes.\",\n",
    "        backstory=\"You synthesize codes into coherent thematic structures.\",\n",
    "        llm=\"gpt-5-mini\",\n",
    "        allow_delegation=False,\n",
    "    )\n",
    "\n",
    "    t1 = Task(\n",
    "        description=f\"\"\"\n",
    "PHASE 1 ‚Äì Familiarization\n",
    "Write analytic memos and patterns.\n",
    "\n",
    "Segments:\n",
    "{json.dumps(dev_segments, ensure_ascii=False)}\n",
    "\"\"\",\n",
    "        expected_output=\"JSON memo.\",\n",
    "        agent=familiarizer,\n",
    "    )\n",
    "\n",
    "    t2 = Task(\n",
    "        description=f\"\"\"\n",
    "PHASE 2 ‚Äì Coding\n",
    "Generate 1‚Äì3 semantic codes per segment.\n",
    "\n",
    "Return JSON:\n",
    "[\n",
    "  {{ \"segment_id\": \"...\", \"post_id\": \"...\", \"codes\": [\"...\"] }}\n",
    "]\n",
    "\n",
    "Segments:\n",
    "{json.dumps(dev_segments, ensure_ascii=False)}\n",
    "\"\"\",\n",
    "        expected_output=\"JSON codes.\",\n",
    "        agent=coder,\n",
    "    )\n",
    "\n",
    "    Crew(\n",
    "        agents=[familiarizer, coder],\n",
    "        tasks=[t1, t2],\n",
    "        process=Process.sequential,\n",
    "    ).kickoff()\n",
    "\n",
    "    segment_codes = parse_json(t2.output.raw)\n",
    "    post_codes = aggregate_codes_to_post(segment_codes)\n",
    "\n",
    "    t3 = Task(\n",
    "        description=f\"\"\"\n",
    "PHASE 3‚Äì5 ‚Äì Theme construction\n",
    "Develop final themes from codes.\n",
    "\n",
    "Codes:\n",
    "{json.dumps(post_codes, ensure_ascii=False)}\n",
    "\n",
    "Return JSON:\n",
    "{{ \"themes\": [{{\"theme_name\": \"...\", \"definition\": \"...\"}}] }}\n",
    "\"\"\",\n",
    "        expected_output=\"JSON themebook.\",\n",
    "        agent=theme_builder,\n",
    "    )\n",
    "\n",
    "    Crew(\n",
    "        agents=[theme_builder],\n",
    "        tasks=[t3],\n",
    "        process=Process.sequential,\n",
    "    ).kickoff()\n",
    "\n",
    "    themebook = parse_json(t3.output.raw)\n",
    "    themes = themebook.get(\"themes\", [])\n",
    "    allowed_themes = {t[\"theme_name\"] for t in themes}\n",
    "\n",
    "    theme_summary = [\n",
    "        {\"theme_name\": t[\"theme_name\"], \"definition\": t[\"definition\"]}\n",
    "        for t in themes\n",
    "    ]\n",
    "\n",
    "    print(f\"‚úî Themebook ready: {len(theme_summary)} themes\")\n",
    "\n",
    "    # =====================================================\n",
    "    # Phase 6: frozen theme application (execution agent)\n",
    "    # =====================================================\n",
    "\n",
    "    filtered_posts = []\n",
    "    for row in df.itertuples():\n",
    "        filtered = filter_by_word_count(row.posts)\n",
    "        if filtered is not None:\n",
    "            filtered_posts.append({\n",
    "                \"post_id\": row.post_id,\n",
    "                \"text\": filtered\n",
    "            })\n",
    "\n",
    "    print(f\"üöÄ Posts after word filter: {len(filtered_posts)}\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i in range(0, len(filtered_posts), apply_batch_size):\n",
    "        batch = filtered_posts[i:i + apply_batch_size]\n",
    "\n",
    "        applier = Agent(\n",
    "            role=\"Theme Application Agent\",\n",
    "            goal=\"Apply fixed themes without reinterpretation.\",\n",
    "            backstory=\"Themes are final; you only assign and extract evidence.\",\n",
    "            llm=\"gpt-5-mini\",\n",
    "            allow_delegation=False,\n",
    "        )\n",
    "\n",
    "        t_apply = Task(\n",
    "            description=f\"\"\"\n",
    "PHASE 6 ‚Äì THEME APPLICATION (FROZEN)\n",
    "\n",
    "Rules:\n",
    "- Use ONLY the provided themebook\n",
    "- Assign EXACTLY ONE primary theme per post\n",
    "- Extract 1‚Äì3 short evidence phrases\n",
    "- Do NOT revise themes\n",
    "- Output STRICT JSON only\n",
    "\n",
    "Return JSON:\n",
    "[\n",
    "  {{\n",
    "    \"post_id\": \"...\",\n",
    "    \"primary_theme\": \"...\",\n",
    "    \"label_phrases\": [\"...\", \"...\"]\n",
    "  }}\n",
    "]\n",
    "\n",
    "Themebook:\n",
    "{json.dumps(theme_summary, ensure_ascii=False)}\n",
    "\n",
    "Posts:\n",
    "{json.dumps(batch, ensure_ascii=False)}\n",
    "\"\"\",\n",
    "            expected_output=\"Strict JSON.\",\n",
    "            agent=applier,\n",
    "        )\n",
    "\n",
    "        Crew(\n",
    "            agents=[applier],\n",
    "            tasks=[t_apply],\n",
    "            process=Process.sequential,\n",
    "            verbose=False,\n",
    "        ).kickoff()\n",
    "\n",
    "        raw = parse_json(t_apply.output.raw, default=[])\n",
    "\n",
    "        for item in raw:\n",
    "            if not isinstance(item, dict):\n",
    "                continue\n",
    "            pid = item.get(\"post_id\")\n",
    "            theme = item.get(\"primary_theme\")\n",
    "            phrases = item.get(\"label_phrases\", [])\n",
    "            if theme not in allowed_themes:\n",
    "                theme = \"Other/Unclear\"\n",
    "            results.append({\n",
    "                \"post_id\": pid,\n",
    "                \"primary_theme\": theme,\n",
    "                \"label_phrases\": phrases\n",
    "            })\n",
    "\n",
    "        print(f\"‚úî Saved batch {i // apply_batch_size + 1}\")\n",
    "\n",
    "    # =====================================================\n",
    "    # Merge back to original df\n",
    "    # =====================================================\n",
    "\n",
    "    labels_df = pd.DataFrame(results)\n",
    "    final_df = df.merge(labels_df, on=\"post_id\", how=\"left\")\n",
    "\n",
    "    final_save_path = save_path.replace(\".csv\", \"_full.csv\")\n",
    "    final_df.to_csv(final_save_path, index=False)\n",
    "\n",
    "    print(f\"üì¶ Final merged file saved to: {final_save_path}\")\n",
    "    print(\"‚úÖ Pipeline completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780315a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_thematic_analysis_with_checkpoint(\n",
    "    csv_path=\"/AI addiction/merged_subreddit_dataset_clean.csv\",\n",
    "    save_path=\"/AI addiction/ALLthematic_clean.csv\",\n",
    "    text_column=\"selftext\",\n",
    "    dev_n=100,\n",
    "    apply_batch_size=20,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
